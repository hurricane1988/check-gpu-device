## ğŸ“Œ é¡¹ç›®ç®€ä»‹
æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªåŸºäº Flask + Gunicorn + NVIDIA CUDA çš„ API æœåŠ¡ï¼Œæä¾› CUDA è®¾å¤‡ä¿¡æ¯æŸ¥è¯¢ å’Œ å¥åº·æ£€æŸ¥ æ¥å£ã€‚æ”¯æŒ GPU è¿è¡Œï¼Œå¯ç”¨äº æ·±åº¦å­¦ä¹ æ¨ç†ç¯å¢ƒ éƒ¨ç½²

---

## âœ¨ åŠŸèƒ½ç‰¹æ€§
- âœ… å¥åº·æ£€æŸ¥ (/healthz) â€”â€” ç¡®ä¿æœåŠ¡æ­£å¸¸è¿è¡Œ
- âœ… CUDA è®¾å¤‡ä¿¡æ¯ (/device) â€”â€” æŸ¥è¯¢ NVIDIA GPU è®¾å¤‡çŠ¶æ€
- âœ… Gunicorn ç”Ÿäº§çº§ WSGI æœåŠ¡å™¨ â€”â€” æä¾›é«˜æ€§èƒ½ API
- âœ… é root è¿è¡Œ â€”â€” æé«˜å®‰å…¨æ€§
- âœ… Docker éƒ¨ç½²æ”¯æŒ â€”â€” é€‚ç”¨äºå®¹å™¨åŒ–ç¯å¢ƒ

---

## ğŸš€ å¿«é€Ÿå¼€å§‹
### 1ï¸âƒ£ æœ¬åœ°è¿è¡Œï¼ˆä»…å¼€å‘ç¯å¢ƒï¼‰
æ‰§è¡Œå¸®å¿™
```shell
make help
```
```shell
Usage:
  make <target>

General
  help             Display this help.

Development
  freeze           Run pip freeze export the python library.
  run              Run a main.py script from your host.

Build
  docker-build     Build docker image with the check-nvidia-cuda.
  docker-push      Push docker image with the check-nvidia-cuda.
  docker-buildx    Build and push docker image for the check-gpu-check for cross-platform support.
```
å®‰è£…ä¾èµ–
```shell
pip install -r requirements.txt
```
å¯åŠ¨æœåŠ¡
```shell
gunicorn -b 0.0.0.0:8000 --access-logfile - main:app
```
è®¿é—® API
```shell
curl http://127.0.0.1:8000/healthz
curl http://127.0.0.1:8000/device
```
---
### 2ï¸âƒ£ Docker è¿è¡Œï¼ˆæ¨èæ–¹å¼ï¼‰
æ„å»º Docker é•œåƒ
```shell
make docker-build
```
è¿è¡Œå®¹å™¨
```shell
docker run --gpus all -p 8000:8000 --rm check-gpu-check
```

```shell
checking nvidia-cuda environment...
âœ… NVIDIA CUDA is available!
+------------------+-------------+
| Property         | Value       |
+==================+=============+
| PyTorch Version  | 2.6.0+cu124 |
+------------------+-------------+
| CUDA Version     | 12.4        |
+------------------+-------------+
| GPU Device Count | 2           |
+------------------+-------------+
+----------+----------+----------------+-------------------+--------------------+-----------------+
|   Device | Name     | Total Memory   | Reserved Memory   | Allocated Memory   | Max Allocated   |
+==========+==========+================+===================+====================+=================+
|        0 | Tesla T4 | 14.58 GB       | 0.00 GB           | 0.00 GB            | 0.00 GB         |
+----------+----------+----------------+-------------------+--------------------+-----------------+
|        1 | Tesla T4 | 14.58 GB       | 0.00 GB           | 0.00 GB            | 0.00 GB         |
+----------+----------+----------------+-------------------+--------------------+-----------------+
```

```shell
curl http://127.0.0.1:8000/device
```
```shell
{
    "cuda_version": "12.4",
    "gpu_count": 2,
    "gpus": [
        {
            "allocated_memory_gb": 0,
            "id": 0,
            "max_allocated_memory_gb": 0,
            "name": "Tesla T4",
            "reserved_memory_gb": 0,
            "total_memory_gb": 14.5775146484375
        },
        {
            "allocated_memory_gb": 0,
            "id": 1,
            "max_allocated_memory_gb": 0,
            "name": "Tesla T4",
            "reserved_memory_gb": 0,
            "total_memory_gb": 14.5775146484375
        }
    ],
    "pytorch_version": "2.6.0+cu124",
    "status": "available"
}
```